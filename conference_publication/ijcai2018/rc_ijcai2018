# Joint Posterior Revision of NLP Annotations via Ontological Knowledge
* **author**ï¼šMarco Rospocher, Francesco Corcoglioniti
* **abstract**: Different well-established NLP tasks contribute to elicit the semantics of entities mentioned in natural language text, such as Named Entity Recognition and Classification (NERC) and Entity Linking (EL). However, combining the outcomes of these tasks may result in NLP annotations --- such as a NERC organization linked by EL to a person --- that are unlikely or contradictory when interpreted in the light of common world knowledge about the entities these annotations refer to. We thus propose a general probabilistic model that explicitly captures the relations between multiple NLP annotations for an entity mention, the ontological entity classes implied by those annotations, and the background ontological knowledge those classes may be consistent with. We use the model to estimate the posterior probability of NLP annotations given their confidences (prior probabilities) and the ontological knowledge, and consequently revise the best annotation choice performed by the NLP tools. In a concrete scenario with two state-of-the-art tools for NERC and EL, we experimentally show on three reference datasets that for these tasks, the joint annotation revision performed by the model consistently improves on the original results of the tools.
* **keywords**: Natural Language Processing: NLP Applications and Tools, Natural Language Processing: Knowledge Extraction, Natural Language Processing: Named Entities
* **interpretation**: 
* **pdf**:  [link](https://www.ijcai.org/Proceedings/2018/0600.pdf)
* **code**: 
* **dataset**: AIDA CoNLL-YAGO, MEANTIME, TAC-KBP
* **ppt/video**: 
* **curator**: Chang Liu
