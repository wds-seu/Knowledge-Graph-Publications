# Knowledge-Enhanced Ensemble Learning for Word Embeddings  
- **author**: Lanting Fang, Yong Luo, Kaiyu Feng, Kaiqi Zhao, Aiqun Hu 
- **abstract**: Representing words as embeddings in a continuous vector space has been proven to be successful in improving the performance in many natural language processing (NLP) tasks. Beyond the traditional methods that learn the embeddings from large text corpora, ensemble methods have been proposed to leverage the merits from pre-trained word embeddings as well as external semantic sources. In this paper, we propose a knowledge-enhanced ensemble method to combine both knowledge graphs and pre-trained word embedding models. Specifically, we interpret relations in knowledge graphs as linear translation from one word to another. We also propose a novel weighting scheme to further distinguish edges in the knowledge graph with same type of relation. Extensive experiments demonstrate that our proposed method is up to 20% times better than state-of-the-art in word analogy task and up to 16% times better than state-of-the-art in word similarity task.
- **keywords**: Word embedding; Knowledgegraph; Ensemble model 
- **interpretation**: 
- **pdf**: [paper](https://github.com/fanglanting/RA-retrofit/blob/master/WWW.pdf)
- **code**: [github](https://github.com/fanglanting/RA-retrofit)
- **dataset**: ConceptNet (CNet), PPDB4 and WordNet (WNet)  
- **ppt/video**:
- **curator**: Xiaoyu Shang 
