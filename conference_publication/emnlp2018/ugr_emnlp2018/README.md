## Joint Multilingual Supervision for Cross-lingual Entity Linking

**author**:Shyam Upadhyay, Nitish Gupta, Dan Roth

**abstract**:Cross-lingual Entity Linking (XEL) aims to
ground entity mentions written in any language to an English Knowledge Base (KB), such as Wikipedia. XEL for most languages is challenging, owing to limited availability of resources as supervision. We address this challenge by developing the first XEL approach
that combines supervision from multiple languages jointly. This enables our approach to: (a) augment the limited supervision in the target language with additional supervision from a high-resource language (like English), and (b) train a single entity linking model for multiple languages, improving upon individually trained models for each language. Extensive evaluation on three benchmark datasets across
8 languages shows that our approach significantly improves over the current state-of-theart. We also provide analyses in two limited resource settings: (a) zero-shot setting, when no supervision in the target language is available, and in (b) low-resource setting, when some supervision in the target language is available. Our analysis provides insights into the limitations of zero-shot XEL approaches in realistic
scenarios, and shows the value of joint supervision in low-resource settings.

**keywords**:

**interpretation**:

**pdf**:[paper](https://www.aclweb.org/anthology/D18-1270.pdf)

**code**:

**dataset**:

**ppt/video**:

**curator**:Ranran Chu