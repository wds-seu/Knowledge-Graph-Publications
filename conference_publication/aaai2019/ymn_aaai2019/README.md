# Combining Axiom Injection and Knowledge Base Completion for Efficient Natural Language Inference  
- **author**: Masashi Yoshikawa, Koji Mineshima, Hiroshi Noji, Daisuke Bekki     
- **abstract**: In logic-based approaches to reasoning tasks such as Recognizing Textual Entailment (RTE), it is important for a system to have a large amount of knowledge data. However, there is a tradeoff between adding more knowledge data for improved RTE performance and maintaining an efficient RTE system, as such a big database is problematic in terms of the memory usage and computational complexity. In this work, we show the processing time of a state-of-the-art logic-based RTE system can be significantly reduced by replacing its search-based axiom injection (abduction) mechanism by that based on Knowledge Base Completion (KBC). We integrate this mechanism in a Coq plugin that provides a proof automation tactic for natural language inference. Additionally, we show empirically that adding new knowledge data contributes to better RTE performance while not harming the processing speed in this framework.
- **keywords**: 
- **interpretation**:
- **pdf**: [paper](https://www.aaai.org/ojs/index.php/AAAI/article/view/4730/4608)
- **code**: [github](https://github.com/masashi-y/abduction_kbc)
- **dataset**: WordNet 
- **ppt/video**:
- **curator**: Xiaoyu Shang 
