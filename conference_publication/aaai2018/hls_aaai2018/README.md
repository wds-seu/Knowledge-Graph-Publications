# Neural Knowledge Acquisition via Mutual Attention Between Knowledge Graph and Text
* **author**: Xu Han, Zhiyuan Liu, Maosong Sun
* **abstract**: We propose a general joint representation learning framework for knowledge acquisition (KA) on two tasks, knowledge graph completion (KGC) and relation extraction (RE) from text. In this framework, we learn representations of knowledge graphs (KGs) and text within a unified parameter sharing semantic space. To achieve better fusion, we propose an effective mutual attention between KGs and text. The reciprocal attention mechanism enables us to highlight important features and perform better KGC and RE. Different from conventional joint models, no complicated linguistic analysis or strict alignments between KGs and text are required to train our models. Experiments on relation extraction and entity link prediction show that models trained under our joint framework are significantly improved in comparison with other baselines. Most existing methods for KGC and RE can be easily integrated into our framework due to its flexible architectures. The source code of this paper can be obtained from https://github.com/thunlp/JointNRE.
* **keywords**: knowledge graph completion, relation extraction
* **interpretation**: [来源: 暂无]()
* **pdf**: [link](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16691/16013)
* **code**: [link](https://github.com/thunlp/JointNRE)
* **dataset**:
* **ppt/video**:
* **curation**: Jiong Zhang
