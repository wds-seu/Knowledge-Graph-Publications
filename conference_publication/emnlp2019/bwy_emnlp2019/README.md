# Incorporating External Knowledge into Machine Reading for Generative Question Answering

- **author**:Bin Bi,Chen Wu,Ming Yan,Wei Wang, Jiangnan Xia,Chenliang Li 
- **abstract**: Commonsense and background knowledge is required for a QA model to answer many nontrivial questions. Different from existing work on knowledge-aware QA, we focus on a more challenging task of leveraging external knowledge to generate answers in natural language for a given question with context. 
In this paper, we propose a new neural model, Knowledge-Enriched Answer Generator (KEAG), which is able to compose a natural answer by exploiting and aggregating evidence from all four information sources available: question, passage, vocabulary and knowledge. During the process of answer generation, KEAG adaptively determines when to utilize symbolic knowledge and which fact from the knowledge is useful. This allows the model to exploit external knowledge that is not explicitly stated in the given text, but that is relevant for generating an answer. The empirical study on public benchmark of answer generation demonstrates that KEAG improves answer quality over models without knowledge and existing knowledge-aware models, confirming its effectiveness in leveraging knowledge.  
- **keywords**:
- **interpretation**:
- **pdf**: [pdf](https://arxiv.org/pdf/1909.02745)
- **code**:
- **dataset**:MARCO
- **ppt/video**:
- **curator**: Yawen Dai