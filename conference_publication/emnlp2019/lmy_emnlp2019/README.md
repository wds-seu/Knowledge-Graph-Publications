# Improving Relation Extraction with Knowledge-attention

- **author**: Pengfei Li, Kezhi Mao, Xuefeng Yang,  Qi Li

- **abstract**: While attention mechanisms have been proven to be effective in many NLP tasks, majority of them are data-driven. We propose a novel knowledge-attention encoder which incorporates prior knowledge from external lexical resources into deep neural networks for relation extraction task. Furthermore, we present three effective ways of integrating knowledge-attention with self-attention to maximize the utilization of both knowledge and data. The proposed relation extraction system is end-to-end and fully attention-based. Experiment results show that the proposed knowledge-attention mechanism has complementary strengths with self-attention, and our integrated models outperform existing CNN, RNN, and self-attention based models. State-of-the-art performance is achieved on TACRED, a complex and large-scale relation extraction dataset.  

- **keywords**:

- **interpretation**:

- **pdf**: [pdf](https://arxiv.org/pdf/1910.02724)

- **code**:

- **dataset**: TACRED

- **ppt/video**:

- **curator**: Yawen Dai