# Knowledge Graph Embedding Compression

* **author**: Mrinmaya Sachan
* **abstract**: Knowledge graph (KG) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications. With a large KG, the embeddings consume a large amount of storage and memory. This is problematic and prohibits the deployment of these techniques in many real world settings. Thus, we propose an approach that compresses the KG embedding layer by representing each entity in the KG as a vector of discrete codes and then composes the embeddings from these codes. The approach can be trained end-to-end with simple modifications to any existing KG embedding technique. We evaluate the approach on various standard KG embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance. The compressed embeddings also retain the ability to perform various reasoning tasks such as KG inference.
* **keywords**:
* **interpretation**:
* **pdf**:[link](https://www.aclweb.org/anthology/2020.acl-main.238.pdf)
* **code**:
* **dataset**:FB15K,FB15k-237,WN18,WN18RR
* **ppt/video**:
* **curator**:Shuwei Yuan

